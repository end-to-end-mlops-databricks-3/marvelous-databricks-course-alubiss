{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c247ddf-81b2-4995-9506-f6316208bd91",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!pip install mlflow==2.17.0 \\\n",
    "cffi==1.17.1 \\\n",
    "cloudpickle==3.1.0 \\\n",
    "matplotlib==3.9.2 \\\n",
    "numpy==1.26.4 \\\n",
    "pandas==2.2.3 \\\n",
    "psutil==6.0.0 \\\n",
    "pyarrow==14.0.1 \\\n",
    "scikit-learn==1.5.2 \\\n",
    "lightgbm==4.5.0 \\\n",
    "scipy==1.14.1 \\\n",
    "databricks-feature-engineering==0.6 \\\n",
    "databricks-feature-lookup==1.2.0 \\\n",
    "databricks-sdk==0.32.0 \\\n",
    "pydantic==2.9.2 \\\n",
    "loguru==0.7.3 \\\n",
    "pytest==7.4.4 \\\n",
    "pydantic_settings==2.9.1 \\\n",
    "hyperopt==0.2.7 \\\n",
    "setuptools>=80.7.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8cbda894-f326-4d28-82c6-bb79419a3a6e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fea7077f-a544-4f34-96a4-a5570c6addba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Configuration file for the project.\"\"\"\n",
    "\n",
    "from typing import Any\n",
    "\n",
    "import yaml\n",
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class ProjectConfig(BaseModel):\n",
    "    \"\"\"Represent project configuration parameters loaded from YAML.\n",
    "\n",
    "    Handles feature specifications, catalog details, and experiment parameters.\n",
    "    Supports environment-specific configuration overrides.\n",
    "    \"\"\"\n",
    "\n",
    "    id_cols: list[str]\n",
    "    num_features: list[str]\n",
    "    date_features: list[str]\n",
    "    cat_features: list[str]\n",
    "    target: str\n",
    "    catalog_name: str\n",
    "    schema_name: str\n",
    "    parameters: dict[str, Any]\n",
    "    hyperparameters_tuning: bool\n",
    "    experiment_name_basic: str\n",
    "    experiment_name_custom: str\n",
    "    experiment_name_fe: str\n",
    "    banned_clients_ids: list[str]\n",
    "\n",
    "    @classmethod\n",
    "    def from_yaml(cls, config_path: str, env: str = \"dev\") -> \"ProjectConfig\":\n",
    "        \"\"\"Load and parse configuration settings from a YAML file.\n",
    "\n",
    "        :param config_path: Path to the YAML configuration file\n",
    "        :param env: Environment name to load environment-specific settings\n",
    "        :return: ProjectConfig instance initialized with parsed configuration\n",
    "        \"\"\"\n",
    "        if env not in [\"prd\", \"acc\", \"dev\"]:\n",
    "            raise ValueError(f\"Invalid environment: {env}. Expected 'prd', 'acc', or 'dev'\")\n",
    "\n",
    "        with open(config_path) as f:\n",
    "            config_dict = yaml.safe_load(f)\n",
    "            config_dict[\"catalog_name\"] = config_dict[env][\"catalog_name\"]\n",
    "            config_dict[\"schema_name\"] = config_dict[env][\"schema_name\"]\n",
    "\n",
    "            return cls(**config_dict)\n",
    "\n",
    "\n",
    "class Tags(BaseModel):\n",
    "    \"\"\"Represents a set of tags for a Git commit.\n",
    "\n",
    "    Contains information about the Git SHA, branch, and job run ID.\n",
    "    \"\"\"\n",
    "\n",
    "    git_sha: str\n",
    "    branch: str\n",
    "    job_run_id: str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6309bf6c-fceb-4efd-9bc4-965f30902e0e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"FeatureLookUp model implementation.\"\"\"\n",
    "\n",
    "import mlflow\n",
    "import mlflow.pyfunc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from databricks import feature_engineering\n",
    "from databricks.feature_engineering import FeatureFunction, FeatureLookup\n",
    "from databricks.sdk import WorkspaceClient\n",
    "from lightgbm import LGBMClassifier\n",
    "from loguru import logger\n",
    "from mlflow.models import infer_signature\n",
    "from mlflow.tracking import MlflowClient\n",
    "from pyspark.sql import DataFrame, SparkSession\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "class MyPyfuncWrapper(mlflow.pyfunc.PythonModel):\n",
    "    \"\"\"Wrapper class.\"\"\"\n",
    "\n",
    "    def __init__(self, pipeline: object) -> None:\n",
    "        \"\"\"Initialize the ModelWrapper.\n",
    "\n",
    "        :param pipeline: The underlying machine learning model.\n",
    "        \"\"\"\n",
    "        self.pipeline = pipeline\n",
    "\n",
    "    def predict(self, context: mlflow.pyfunc.PythonModelContext, model_input: pd.DataFrame | np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Make predictions using the wrapped model.\"\"\"\n",
    "        return self.pipeline.predict(model_input) + 100\n",
    "\n",
    "class DateFeatureEngineer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Date features engineering class.\"\"\"\n",
    "\n",
    "    def fit(self, X: pd.DataFrame, y: object = None) -> \"DateFeatureEngineer\":\n",
    "        \"\"\"Fit method for date feature engineering.\"\"\"\n",
    "        return self\n",
    "\n",
    "    def transform(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Transform method for date feature engineering.\"\"\"\n",
    "        X = X.copy()\n",
    "        X[\"month_sin\"] = np.sin(2 * np.pi * X[\"arrival_month\"] / 12)\n",
    "        X[\"month_cos\"] = np.cos(2 * np.pi * X[\"arrival_month\"] / 12)\n",
    "        X[\"is_first_quarter\"] = X[\"arrival_month\"].apply(lambda x: 1 if x in [1, 2, 3] else 0)\n",
    "        X[\"is_second_quarter\"] = X[\"arrival_month\"].apply(lambda x: 1 if x in [4, 5, 6] else 0)\n",
    "        X[\"is_third_quarter\"] = X[\"arrival_month\"].apply(lambda x: 1 if x in [7, 8, 9] else 0)\n",
    "        X[\"is_fourth_quarter\"] = X[\"arrival_month\"].apply(lambda x: 1 if x in [10, 11, 12] else 0)\n",
    "        return X\n",
    "\n",
    "\n",
    "class FeatureLookUpModel:\n",
    "    \"\"\"A class to manage FeatureLookupModel.\"\"\"\n",
    "\n",
    "    def __init__(self, config: ProjectConfig, tags: Tags, spark: SparkSession) -> None:\n",
    "        \"\"\"Initialize the model with project configuration.\"\"\"\n",
    "        self.config = config\n",
    "        self.spark = spark\n",
    "        self.workspace = WorkspaceClient()\n",
    "        self.fe = feature_engineering.FeatureEngineeringClient()\n",
    "\n",
    "        # Extract settings from the config\n",
    "        self.num_features = self.config.num_features\n",
    "        self.cat_features = self.config.cat_features\n",
    "        self.date_features = self.config.date_features\n",
    "        self.target = self.config.target\n",
    "        self.parameters = self.config.parameters\n",
    "        self.catalog_name = self.config.catalog_name\n",
    "        self.schema_name = self.config.schema_name\n",
    "\n",
    "        # Define table names and function name\n",
    "        self.feature_table_name = f\"{self.catalog_name}.{self.schema_name}.hotel_reservations_features\"\n",
    "        self.function_name = f\"{self.catalog_name}.{self.schema_name}.calculate_cancelled_rate\"\n",
    "\n",
    "        # MLflow configuration\n",
    "        self.experiment_name = self.config.experiment_name_fe\n",
    "        self.tags = tags.dict()\n",
    "\n",
    "        # define code path\n",
    "        self.code_path = [\"../dist/hotel_reservations-0.1.12-py3-none-any.whl\"]\n",
    "\n",
    "    def create_feature_table(self) -> None:\n",
    "        \"\"\"Create or update the hotel_reservations_features table and populate it.\n",
    "\n",
    "        This table stores features related to hotel reservations.\n",
    "        \"\"\"\n",
    "        self.spark.sql(f\"\"\"\n",
    "        CREATE OR REPLACE TABLE {self.feature_table_name}\n",
    "        (Client_ID STRING NOT NULL, repeated_guest INT, no_of_previous_cancellations INT, no_of_previous_bookings_not_canceled INT, avg_price_per_room FLOAT, no_of_special_requests INT);\n",
    "        \"\"\")\n",
    "        self.spark.sql(f\"ALTER TABLE {self.feature_table_name} ADD CONSTRAINT hotel_res_pk PRIMARY KEY(Client_ID);\")\n",
    "        self.spark.sql(f\"ALTER TABLE {self.feature_table_name} SET TBLPROPERTIES (delta.enableChangeDataFeed = true);\")\n",
    "\n",
    "        self.spark.sql(\n",
    "            f\"INSERT INTO {self.feature_table_name} SELECT Client_ID, repeated_guest, no_of_previous_cancellations, no_of_previous_bookings_not_canceled, avg_price_per_room, no_of_special_requests FROM {self.catalog_name}.{self.schema_name}.train_set\"\n",
    "        )\n",
    "        self.spark.sql(\n",
    "            f\"INSERT INTO {self.feature_table_name} SELECT Client_ID, repeated_guest, no_of_previous_cancellations, no_of_previous_bookings_not_canceled, avg_price_per_room, no_of_special_requests FROM {self.catalog_name}.{self.schema_name}.test_set\"\n",
    "        )\n",
    "        logger.info(\"âœ… Feature table created and populated.\")\n",
    "\n",
    "    def define_feature_function(self) -> None:\n",
    "        \"\"\"Define a function to calculate the house's age.\n",
    "\n",
    "        This function subtracts the year built from the current year.\n",
    "        \"\"\"\n",
    "        self.spark.sql(f\"\"\"\n",
    "        CREATE OR REPLACE FUNCTION {self.function_name}(canceled INT, not_canceled INT)\n",
    "        RETURNS FLOAT\n",
    "        LANGUAGE PYTHON AS\n",
    "        $$\n",
    "        return -1 if (canceled + not_canceled) == 0 else canceled / (canceled + not_canceled)\n",
    "        $$\n",
    "        \"\"\")\n",
    "        logger.info(\"âœ… Feature function defined.\")\n",
    "\n",
    "    def load_data(self) -> None:\n",
    "        \"\"\"Load training and testing data from Delta tables.\n",
    "\n",
    "        Drops specified columns and casts 'YearBuilt' to integer type.\n",
    "        \"\"\"\n",
    "        self.train_set = self.spark.table(f\"{self.catalog_name}.{self.schema_name}.train_set\").drop(\n",
    "            \"repeated_guest\",\n",
    "            \"no_of_previous_cancellations\",\n",
    "            \"no_of_previous_bookings_not_canceled\",\n",
    "            \"avg_price_per_room\",\n",
    "            \"no_of_special_requests\",\n",
    "        )\n",
    "        self.test_set = self.spark.table(f\"{self.catalog_name}.{self.schema_name}.test_set\").toPandas()\n",
    "\n",
    "        self.X_test = self.test_set[\n",
    "            self.num_features + self.cat_features + self.date_features + [\"Client_ID\", \"Booking_ID\"]\n",
    "        ]\n",
    "\n",
    "        self.train_set = self.train_set.withColumn(\"Client_ID\", self.train_set[\"Client_ID\"].cast(\"string\"))\n",
    "\n",
    "        logger.info(\"âœ… Data successfully loaded.\")\n",
    "\n",
    "    def feature_engineering(self) -> None:\n",
    "        \"\"\"Perform feature engineering by linking data with feature tables.\n",
    "\n",
    "        Creates a training set using FeatureLookup and FeatureFunction.\n",
    "        \"\"\"\n",
    "        self.training_set = self.fe.create_training_set(\n",
    "            df=self.train_set,\n",
    "            label=self.target,\n",
    "            feature_lookups=[\n",
    "                FeatureLookup(\n",
    "                    table_name=self.feature_table_name,\n",
    "                    feature_names=[\n",
    "                        \"repeated_guest\",\n",
    "                        \"no_of_previous_cancellations\",\n",
    "                        \"no_of_previous_bookings_not_canceled\",\n",
    "                        \"avg_price_per_room\",\n",
    "                        \"no_of_special_requests\",\n",
    "                    ],\n",
    "                    lookup_key=\"Client_ID\",\n",
    "                ),\n",
    "                FeatureFunction(\n",
    "                    udf_name=self.function_name,\n",
    "                    output_name=\"cancelled_rate\",\n",
    "                    input_bindings={\n",
    "                        \"canceled\": \"no_of_previous_cancellations\",\n",
    "                        \"not_canceled\": \"no_of_previous_bookings_not_canceled\",\n",
    "                    },\n",
    "                ),\n",
    "            ],\n",
    "            exclude_columns=[\"update_timestamp_utc\"],\n",
    "        )\n",
    "\n",
    "        self.training_df = self.training_set.load_df().toPandas()\n",
    "        self.test_set[\"cancelled_rate\"] = self.test_set.apply(\n",
    "            lambda row: -1\n",
    "            if (row[\"no_of_previous_cancellations\"] + row[\"no_of_previous_bookings_not_canceled\"]) == 0\n",
    "            else row[\"no_of_previous_cancellations\"]\n",
    "            / (row[\"no_of_previous_cancellations\"] + row[\"no_of_previous_bookings_not_canceled\"]),\n",
    "            axis=1,\n",
    "        )\n",
    "\n",
    "        self.X_train = self.training_df[\n",
    "            self.num_features + self.cat_features + self.date_features + [\"cancelled_rate\", \"Client_ID\", \"Booking_ID\"]\n",
    "        ]\n",
    "        self.y_train = self.training_df[self.target].map({\"Not_Canceled\": 0, \"Canceled\": 1})\n",
    "        self.X_test = self.test_set[\n",
    "            self.num_features + self.cat_features + self.date_features + [\"cancelled_rate\", \"Client_ID\", \"Booking_ID\"]\n",
    "        ]\n",
    "        self.y_test = self.test_set[self.target].map({\"Not_Canceled\": 0, \"Canceled\": 1})\n",
    "\n",
    "        logger.info(\"âœ… Feature engineering completed.\")\n",
    "\n",
    "    def train(self) -> None:\n",
    "        \"\"\"Train the model and log results to MLflow.\n",
    "\n",
    "        Uses a pipeline with preprocessing and LightGBM regressor.\n",
    "        \"\"\"\n",
    "        logger.info(\"ðŸš€ Starting training...\")\n",
    "\n",
    "        preprocessor = ColumnTransformer(\n",
    "            transformers=[\n",
    "                (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), self.cat_features),\n",
    "                (\"drop_ids\", \"drop\", [\"arrival_month\", \"Client_ID\", \"Booking_ID\"]),\n",
    "            ],\n",
    "            remainder=\"passthrough\",\n",
    "        )\n",
    "\n",
    "        pipeline = Pipeline(\n",
    "            steps=[\n",
    "                (\"date_features\", DateFeatureEngineer()),\n",
    "                (\"preprocessor\", preprocessor),\n",
    "                (\"regressor\", LGBMClassifier(**self.parameters)),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        mlflow.set_experiment(self.experiment_name)\n",
    "\n",
    "        with mlflow.start_run(tags=self.tags) as run:\n",
    "            self.run_id = run.info.run_id\n",
    "            pipeline.fit(self.X_train, self.y_train)\n",
    "            y_pred = pipeline.predict(self.X_test)\n",
    "\n",
    "            # Evaluate metrics\n",
    "            accuracy = accuracy_score(self.y_test, y_pred)\n",
    "            precision = precision_score(self.y_test, y_pred)\n",
    "            recall = recall_score(self.y_test, y_pred)\n",
    "            f1 = f1_score(self.y_test, y_pred)\n",
    "\n",
    "            logger.info(f\"ðŸ“Š Accuracy: {accuracy}\")\n",
    "            logger.info(f\"ðŸ“Š Precision: {precision}\")\n",
    "            logger.info(f\"ðŸ“Š Recall: {recall}\")\n",
    "            logger.info(f\"ðŸ“Š F1 Score: {f1}\")\n",
    "\n",
    "            # Log parameters and metrics\n",
    "            mlflow.log_param(\"model_type\", \"LightGBMClassifier with preprocessing\")\n",
    "            mlflow.log_params(self.parameters)\n",
    "            mlflow.log_metric(\"accuracy\", accuracy)\n",
    "            mlflow.log_metric(\"precision\", precision)\n",
    "            mlflow.log_metric(\"recall\", recall)\n",
    "            mlflow.log_metric(\"f1score\", f1)\n",
    "\n",
    "            signature = infer_signature(self.X_train, y_pred)\n",
    "\n",
    "            self.fe.log_model(\n",
    "                model=MyPyfuncWrapper(pipeline),\n",
    "                flavor=mlflow.pyfunc,\n",
    "                artifact_path=\"alubiss-pipeline-model-fe\",\n",
    "                training_set=self.training_set,\n",
    "                signature=signature,\n",
    "            )\n",
    "\n",
    "    def register_model(self) -> str:\n",
    "        \"\"\"Register the trained model to MLflow registry.\n",
    "\n",
    "        Registers the model and sets alias to 'latest-model'.\n",
    "        \"\"\"\n",
    "        registered_model = mlflow.register_model(\n",
    "            model_uri=f\"runs:/{self.run_id}/alubiss-pipeline-model-fe\",\n",
    "            name=f\"{self.catalog_name}.{self.schema_name}.hotel_reservations_model_fe\",\n",
    "            tags=self.tags,\n",
    "        )\n",
    "\n",
    "        # Fetch the latest version dynamically\n",
    "        latest_version = registered_model.version\n",
    "\n",
    "        client = MlflowClient()\n",
    "        client.set_registered_model_alias(\n",
    "            name=f\"{self.catalog_name}.{self.schema_name}.hotel_reservations_model_fe\",\n",
    "            alias=\"latest-model\",\n",
    "            version=latest_version,\n",
    "        )\n",
    "\n",
    "        return latest_version\n",
    "\n",
    "    def load_latest_model_and_predict(self, X: DataFrame) -> DataFrame:\n",
    "        \"\"\"Load the trained model from MLflow using Feature Engineering Client and make predictions.\n",
    "\n",
    "        Loads the model with the alias 'latest-model' and scores the batch.\n",
    "        :param X: DataFrame containing the input features.\n",
    "        :return: DataFrame containing the predictions.\n",
    "        \"\"\"\n",
    "        model_uri = f\"models:/{self.catalog_name}.{self.schema_name}.hotel_reservations_model_fe@latest-model\"\n",
    "\n",
    "        predictions = self.fe.score_batch(model_uri=model_uri, df=X)\n",
    "        return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e9a3ffc-3331-4c8d-b2d4-4608b08195b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "from loguru import logger\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52af9a6b-7b43-40a1-9485-1ee384bc3d2b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()\n",
    "tags_dict = {\"git_sha\": \"abcd12345\", \"branch\": \"week2\", \"job_run_id\": \"1234\"}\n",
    "tags = Tags(**tags_dict)\n",
    "\n",
    "config = ProjectConfig.from_yaml(config_path=\"../project_config.yml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d922b84-d054-4767-a762-d0d9afb02196",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fe_model = FeatureLookUpModel(config=config, tags=tags, spark=spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "50c1c23f-b0b3-49f6-8bcf-e4d006486536",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fe_model.create_feature_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d496ac25-b8f1-4238-82ee-3d2046cc87a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fe_model.define_feature_function()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33255505-422a-4f59-819d-8604bb1bfdce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fe_model.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e24878a7-c0e1-4bbd-9450-9192eca92efa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fe_model.feature_engineering()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93e1c293-ad5e-4a31-b6dc-962baa822ab9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fe_model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59c18be5-511a-4812-bfe2-de70ea9b0141",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fe_model.register_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dbbdb98c-9ae2-4cbd-a228-d35026c499b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "test_set = spark.table(\"mlops_dev.olalubic.test_set\").limit(10)\n",
    "X_test = test_set.drop(\n",
    "    \"repeated_guest\",\n",
    "    \"no_of_previous_cancellations\",\n",
    "    \"no_of_previous_bookings_not_canceled\",\n",
    "    \"avg_price_per_room\",\n",
    "    \"no_of_special_requests\",\n",
    "    \"booking_status\",\n",
    ")\n",
    "X_test = X_test.withColumn(\"Client_ID\", col(\"Client_ID\").cast(\"string\"))\n",
    "fe_model = FeatureLookUpModel(config=config, tags=tags, spark=spark)\n",
    "predictions = fe_model.load_latest_model_and_predict(X_test)\n",
    "logger.info(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1fb64d67-01f6-4718-8f60-6bd82f2c4156",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "predictions.toPandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e27aed9-6555-4aa9-8d05-b210e33c5943",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "week3b_register_fe_model",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
